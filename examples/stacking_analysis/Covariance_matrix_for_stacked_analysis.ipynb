{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we import some clmm modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import clmm\n",
    "from itertools import combinations, chain\n",
    "import healpy\n",
    "import clmm.dataops\n",
    "from clmm.dataops import compute_tangential_and_cross_components, make_radial_profile, make_bins\n",
    "from clmm.galaxycluster import GalaxyCluster\n",
    "import clmm.utils as u\n",
    "from clmm import Cosmology\n",
    "from clmm.support import mock_data as mock\n",
    "cosmo = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)\n",
    "import numpy as np\n",
    "cosmo_clmm = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from numpy import random\n",
    "import scipy\n",
    "\n",
    "clmm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we import `clmm` module's core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define `astropy` and `ccl` cosmology object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_cosmo = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moo = clmm.Modeling(massdef = 'critical', delta_mdef = 200, halo_profile_model = 'nfw')\n",
    "moo.set_cosmo(mock_cosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the `Modeling` object `moo` to model the galaxy cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(corner): return [[corner[i],corner[i+1]] for i in range(len(corner)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator of the stacked excess surface density\n",
    "\n",
    "- The maximum likelihood estimator of the excess surface density in the radial bin $[R, R + \\Delta R[$ is defined by\n",
    "\n",
    "$$\\widehat{\\Delta\\Sigma}_+(R) = \\frac{1}{\\sum\\limits_{l,s = 1} w_{ls}}\n",
    "     \\sum\\limits_{l,s= 1}w_{ls}\\langle\\Sigma_{{\\rm crit}}(z_s, z_l)^{-1}\\rangle^{-1}\\epsilon_+^{s},$$ where $\\epsilon_+^{s}$ is the tangential ellipticity of the background galaxy with index $s$ (as source) relative to the dark matter halo position with index $l$ (as lens).\n",
    "     \n",
    "- The critical surface mass density expresses as $\\Sigma_{{\\rm crit}}(z_s, z_l) = \\frac{c^2}{4 \\pi G} \\frac{D_A(z_s)}{D_A(z_l) D_A(z_s, z_l)}$, where $D_A(z_l), D_A(z_s)$ are respectively the angular diameter distance to the lens and to the source in physical units, and $D_A(z_s, z_l)$ is the angular diameter distance between the lens and the source.\n",
    "\n",
    "\n",
    "- The average $\\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle$ is defined as $\\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle = \\int_{z_l + \\delta}^{+\\infty} d z_s\\ p_{\\rm photoz,s}(z_s)\\ \\Sigma_{\\rm crit}(z_s,z_l)^{-1}$, where $p_{\\rm photoz,s}$ is the photometric probability density function for the background galaxy with index $s$.\n",
    "\n",
    "This equation can be re-written as\n",
    "\n",
    "$$\\widehat{\\Delta\\Sigma}_+(R) = \\frac{1}{\\sum\\limits_{l = 1} W_{l}}\n",
    "     \\sum\\limits_{l,s= 1}W_{l}\\Delta\\Sigma_l(R)$$\n",
    "     \n",
    "where $$\\Delta\\Sigma_l(R) = \\frac{1}{\\sum\\limits_{s = 1} w_{ls}}\n",
    "     \\sum\\limits_{s= 1}w_{ls}\\langle\\Sigma_{{\\rm crit}}(z_s, z_l)^{-1}\\rangle^{-1}\\epsilon_+^{s}$$\n",
    "     \n",
    "and $$W_{l} = \\sum\\limits_{s= 1}w_{ls}$$\n",
    "\n",
    "The quantities $w_{s}$ are the weights that maximise the sigmnal-to-noise ratio of the excess surface density estimator. They downweight the galaxies that are close in redshift to the cluster (where the lensing signal is weak). They include the lack of informations on both redshift and shape reconstruction for each background galaxies. \n",
    "\n",
    "- In the case where there is no error on the shape measurement (for the purpose of cosmoDC2 galaxies), the weight writes $w_{ls} = \\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle^{2}$,\n",
    "\n",
    "- In the case of true redshift $z_s$, the PDF reduces to a Dirac function centered at $z_s$, giving the average $\\langle\\Sigma_{\\rm crit}(z_l,z_s)^{-1}\\rangle = \\Sigma_{\\rm crit}(z_l,z_s)^{-1} $ and the weight $w_{s} = \\Sigma_{\\rm crit}(z_s,z_l)^{-2}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define some redshift interval and input mass interval to select GalaxyCluster objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dark matter halo catalog for the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bin = [0.2,0.25]\n",
    "logm_bin = [14, 14.1] #Solar Mass\n",
    "logm_bin = np.array(logm_bin)\n",
    "n_catalogs = 50\n",
    "ngals = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define a set of selected cluster with given true masses ans true redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_m = 10**((logm_bin[1] - logm_bin[0])*np.random.random(n_catalogs) + logm_bin[0]) #in M_sun\n",
    "cluster_z = (z_bin[1] - z_bin[0])*np.random.random(n_catalogs) + z_bin[0]\n",
    "lnc = abs(np.log(4) + 0.1*np.random.randn(n_catalogs))\n",
    "concentration = np.exp(lnc)\n",
    "#generate random position on the sky\n",
    "ra = np.random.random(n_catalogs)*(360 + 0) - 0 #from 0 to 360 deg\n",
    "sindec = np.random.random(n_catalogs)*(1 + 1) - 1\n",
    "dec = np.arcsin(sindec)*180/np.pi #from -90 to 90 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (14,5))\n",
    "ax[0].scatter(cluster_m, concentration)\n",
    "ax[0].set_xlabel(r'$M_{\\rm 200c}$', fontsize = 20)\n",
    "ax[0].set_ylabel(r'$c_{\\rm 200c}$', fontsize = 20)\n",
    "ax[0].set_title('c-M relation', fontsize = 20)\n",
    "ax[1].scatter(ra, dec)\n",
    "ax[1].set_xlabel(r'$ra\\ [deg]$', fontsize = 20)\n",
    "ax[1].set_ylabel(r'$dec\\ [deg]$', fontsize = 20)\n",
    "ax[1].set_title('2D map', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a given shapenoise for background galaxy shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenoise = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_surface_density(single_catalog = None, radial_bin = None, sigma_c = None):\n",
    "    r\"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    \n",
    "    single_catalog : GalaxyCluster object\n",
    "    \n",
    "    radial_bin : liste\n",
    "        radial bins to evaluate the binned excess surface density\n",
    "    \n",
    "    sigma_c : string \n",
    "        column name in the single_catalog for the critical surface mass density\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    ds, r, sum_weights : array, array, array\n",
    "        the binned excess surface density, the binned radius, the sum of weights w_ls in each radial bin\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ds = np.zeros(len(radial_bin))\n",
    "    \n",
    "    r = np.zeros(len(radial_bin))\n",
    "    \n",
    "    sum_weights = np.zeros(len(radial_bin))\n",
    "    \n",
    "    n_gal_per_bin = np.zeros(len(radial_bin))\n",
    "    \n",
    "    for i, r_bin in enumerate(radial_bin):\n",
    "\n",
    "        mask = (single_catalog.galcat['R'] > r_bin[0])*(single_catalog.galcat['R'] < r_bin[1])\n",
    "        \n",
    "        w_ls = 1./single_catalog.galcat[sigma_c][mask]**2\n",
    "        \n",
    "        ds[i] = np.average(single_catalog.galcat['et'][mask]*single_catalog.galcat[sigma_c][mask], weights = w_ls)\n",
    "        \n",
    "        r[i] = np.average(single_catalog.galcat['R'][mask])\n",
    "        \n",
    "        n_gal_per_bin[i] = len(mask[mask == True])\n",
    "        \n",
    "        sum_weights[i] = np.sum( w_ls )\n",
    "        \n",
    "    return ds, r, sum_weights, n_gal_per_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating background galaxy catalogs & estimating individual shear profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `clmm` to generate `n_catalogs` background galaxy catalogs for each clusters in the sample. We estimate the individual shear profile and store the individual data:\n",
    "\n",
    "- `ds_single`, `r_single` the single excess surface density extimated in the radial bins, and the average radius in each radial bins\n",
    "- `W_l` the sum of the weight `w_ls` for each lens-source pairs in each radial bins. Here, we consider true redshifts of background galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_photoz_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "GalaxyCluster_list = []\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    if i%10 == 0: print(i)\n",
    "    \n",
    "    noisy_data_z = mock.generate_galaxy_catalog(cluster_m[i], cluster_z[i], concentration[i], \n",
    "                                                cosmo, \n",
    "                                                zsrc = 'chang13', \n",
    "                                                delta_so=200, \n",
    "                                                massdef='critical',\n",
    "                                                halo_profile_model='nfw', \n",
    "                                                zsrc_min=cluster_z[i] + 0.1,\n",
    "                                                zsrc_max=3., \n",
    "                                                field_size=10., \n",
    "                                                shapenoise=shapenoise, \n",
    "                                                photoz_sigma_unscaled=0.05, \n",
    "                                                ngals=ngals)\n",
    "    \n",
    "    if use_photoz_weights == True:\n",
    "        \n",
    "        pzbins_constant = np.linspace(0, 3, 30)[1:]\n",
    "        \n",
    "        sigma_crit_1 = 1./mock_cosmo.eval_sigma_crit(cluster_z[i], pzbins_constant)\n",
    "\n",
    "        photoz_matrix = np.zeros([len(noisy_data_z), len(pzbins_constant)])\n",
    "        \n",
    "        r\"\"\"\n",
    "        I added this line since the photoz pdf is not computed on a constant redshift grid. simps integral can be optimized using the same x-axis for multiple integrals.\n",
    "        =================================================================================================================================================================\n",
    "        \"\"\"\n",
    "\n",
    "        for f in range(len(noisy_data_z['id'])):\n",
    "\n",
    "            photoz_matrix[f,:] = np.interp(pzbins_constant, noisy_data_z['pzbins'][f], noisy_data_z['pzpdf'][f])\n",
    "            \n",
    "        r\"\"\"\n",
    "        =================================================================================================================================================================\n",
    "        \"\"\"\n",
    "\n",
    "        unormed_integral = scipy.integrate.simps(photoz_matrix * sigma_crit_1, x = pzbins_constant, axis = 1)\n",
    "\n",
    "        norm = scipy.integrate.simps(photoz_matrix, x = pzbins_constant, axis = 1)\n",
    "\n",
    "        noisy_data_z['sigma_c_photoz'] = (unormed_integral/norm)**(-1)\n",
    "        \n",
    "    \n",
    "    cl = clmm.GalaxyCluster('mock_cluster', 0, 0, cluster_z[i], noisy_data_z)\n",
    "    \n",
    "    cl.compute_tangential_and_cross_components(geometry=\"flat\",\n",
    "                                              shape_component1='e1', shape_component2='e2', \n",
    "                                              tan_component='et', cross_component='ex',\n",
    "                                              add=True, \n",
    "                                              is_deltasigma = False, \n",
    "                                              cosmo = cosmo)\n",
    "    \n",
    "    cl.galcat['R'] = mock_cosmo.eval_da_z1z2(0,cluster_z[i])*cl.galcat['theta']\n",
    "    \n",
    "    ang, phi = clmm.dataops._compute_lensing_angles_flatsky(cl.ra, cl.dec, cl.galcat['ra'], cl.galcat['dec'])\n",
    "\n",
    "    cl.galcat['phi'] = np.array(phi)\n",
    "    \n",
    "    cl.galcat['sigma_c_true'] = mock_cosmo.eval_sigma_crit(cluster_z[i], cl.galcat['ztrue'])\n",
    "    \n",
    "    GalaxyCluster_list.append(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked profile\n",
    "$$\\widehat{\\Delta\\Sigma}_+(R) = \\frac{1}{\\sum\\limits_{l = 1} W_{l}}\n",
    "     \\sum\\limits_{l= 1}W_{l}\\Delta\\Sigma_l(R)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bins = np.logspace(np.log10(0.3), np.log10(5), 20)\n",
    "radial_bin = [[new_bins[s],new_bins[s+1]] for s in range(len(new_bins)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ds_single', 'r_single', 'W_l', 'N_gal_per_bin', 'c', 'z', 'mass', 'ra', 'dec']\n",
    "\n",
    "Stack_file_true = {name : [] for name in names}\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    ds_single, r_single, sum_weights_single, N_gal_per_bin = excess_surface_density(single_catalog = GalaxyCluster_list[i], radial_bin = radial_bin, sigma_c = 'sigma_c_true')\n",
    "    \n",
    "    data_to_store = [ds_single, r_single, sum_weights_single, N_gal_per_bin, concentration[i], cluster_z[i], cluster_m[i], ra[i], dec[i]]\n",
    "    \n",
    "    for d, name in enumerate(names):\n",
    "        \n",
    "        Stack_file_true[name].append(data_to_store[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the mean excess surface density profile `ds_mean` and the mean radius `r_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked excess surface density profile - true redshift case\n",
    "\n",
    "ds_mean_true = np.average(Stack_file_true['ds_single'], weights = Stack_file_true['W_l'], axis = 0)\n",
    "r_mean_true = np.average(Stack_file_true['r_single'], weights = None, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix for stacked analysis\n",
    "## Sample covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the sample covariance matrix defined as\n",
    "$$\n",
    "    (\\mathbf{C}^{\\rm sample})_{i,j} = \\frac{1}{N_{\\rm cluster} - 1}\\sum_{k = 0}^{N_{\\rm cluster}}[\\Delta\\Sigma^k(R_i) - \\overline{\\Delta\\Sigma}(R_i)] [\\Delta\\Sigma^k(R_j) - \\overline{\\Delta\\Sigma}(R_j)],\n",
    "$$\n",
    "and $\\overline{\\Delta\\Sigma}(R_i)$ is denoted by\n",
    "$$\n",
    "    \\overline{\\Delta\\Sigma}(R_i) = \\frac{1}{N_{\\rm cluster}}\\sum_{k = 0}^{N_{\\rm cluster}} \\Delta\\Sigma^k(R_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_covariance_stack_true = np.cov(np.array(Stack_file_true['ds_single']).T)/n_catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife covariance matrix\n",
    "We also estimate the covariance matrix by the delete-1 Jackknife method : We split the full sky area arroundin $N_{\\rm JK}$ regions, and we exclude one region at a time and re-estimate the stacked excess surface density profile:, the covariance writes\n",
    "\n",
    "$$\n",
    "    (\\mathbf{C}^{\\rm JK})_{i,j} = \\frac{N_{\\rm JK} - 1}{N_{\\rm JK}}\\sum_{k = 0}^{N_{\\rm JK}}[\\Delta\\Sigma^k(R_i) - \\overline{\\Delta\\Sigma}(R_i)] [\\Delta\\Sigma^k(R_j) - \\overline{\\Delta\\Sigma}(R_j)]\n",
    "$$\n",
    "with the mean \n",
    "$$\n",
    "    \\overline{\\Delta\\Sigma}(R_i) = \\frac{1}{N_{\\rm JK}}\\sum_{k = 0}^{N_{\\rm JK}} \\Delta\\Sigma^k(R_i).\n",
    "$$\n",
    "For this example, we use the sky sub-division of the 3D sphere according to the healpix pixelisation (package `healpy`). It depends on the parameter `n_side`, leading to a sky area divided in `12*n_side**2` sub-regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_jackknife_covariance_matrix(file, ra_colname, dec_colname, n_side, N_delete):\n",
    "    \n",
    "        r\"\"\"\n",
    "        Attributes:\n",
    "        -----------\n",
    "        file : dictionary, astropy Table\n",
    "            file with the individual excess surface density profiles\n",
    "        ra_colname, dec_colname : string, string\n",
    "            names of the columns in the dictionnary that corresponds to the ra and dec coordinates\n",
    "        n_side : int (2, 4, 8)\n",
    "            parameter for the healpix pixel sub-division of the 3D sphere\n",
    "        N_delete : int\n",
    "            number of removed regions for each realization (delete-d jackknife method)\n",
    "        Returns:\n",
    "        --------\n",
    "        cov : array\n",
    "            Jackknife delete-d covariance matrix\n",
    "        \"\"\"\n",
    "    \n",
    "        ra, dec =  file[ra_colname], file[dec_colname]\n",
    "        \n",
    "        index = np.arange(len(ra))\n",
    "\n",
    "        healpix = healpy.ang2pix(2**n_side, ra, dec, nest=True, lonlat=True)\n",
    "        \n",
    "        healpix_list_unique = np.unique(healpix)\n",
    "        \n",
    "        #for h in healpix_list_unique: plt.scatter(np.array(ra)[healpix == h], np.array(dec)[healpix == h])\n",
    "        \n",
    "        healpix_combination_delete = list(combinations(healpix_list_unique, N_delete))\n",
    "\n",
    "        data_jack = []\n",
    "\n",
    "        for i, hp_list_delete in enumerate(healpix_combination_delete):\n",
    "\n",
    "                mask_in_area = np.isin(healpix, hp_list_delete)\n",
    "\n",
    "                mask_out_area = np.invert(mask_in_area)\n",
    "\n",
    "                file_jackknife = Table(file)[mask_out_area]\n",
    "                \n",
    "                ds_mean_jackknife = np.average(file_jackknife['ds_single'], weights = file_jackknife['W_l'], axis = 0)\n",
    "                \n",
    "                data_jack.append(ds_mean_jackknife)\n",
    "\n",
    "        data_jack = np.array(data_jack)\n",
    "\n",
    "        N = np.stack((data_jack.astype(float)), axis = 1)\n",
    "        \n",
    "        n_jack = len(healpix_combination_delete)\n",
    "\n",
    "        cov_N = (n_jack - 1) * np.cov(N, bias = False, ddof=0)\n",
    "\n",
    "        coeff = (n_jack-N_delete)/(N_delete*n_jack)\n",
    "        \n",
    "        return cov_N * coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_side = 2\n",
    "n_jack = 12*n_side**2\n",
    "jackknife_covariance_stack_true = stack_jackknife_covariance_matrix(Stack_file_true,  'ra', 'dec', n_side, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap covariance matrix\n",
    "We also estimate the covariance matrix by the bootstrap method : we create $N_{\\rm boot}$ resamplings of the cluster stack, with the same number of clusters but by taking them by random sampling with replacement.\n",
    "\n",
    "$$\n",
    "    (\\mathbf{C}^{\\rm boot})_{i,j} = \\frac{1}{N_{\\rm boot}-1}\\sum_{k = 0}^{N_{\\rm boot}}[\\Delta\\Sigma^k(R_i) - \\overline{\\Delta\\Sigma}(R_i)] [\\Delta\\Sigma^k(R_j) - \\overline{\\Delta\\Sigma}(R_j)]\n",
    "$$\n",
    "with the mean \n",
    "$$\n",
    "    \\overline{\\Delta\\Sigma}(R_i) = \\frac{1}{N_{\\rm boot}}\\sum_{k = 0}^{N_{\\rm boot}} \\Delta\\Sigma^k(R_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_bootstrap_covariance_matrix(file, n_boot):\n",
    "    \n",
    "        r\"\"\"\n",
    "        Attributes:\n",
    "        -----------\n",
    "        file : dictionary, astropy Table\n",
    "            file with the individual excess surface density profiles\n",
    "        n_boot : int\n",
    "            number of bootstrap resampling resampling\n",
    "        Returns:\n",
    "        --------\n",
    "        cov : array\n",
    "            Jackknife delete-d covariance matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        index_halos = np.arange(len(file['ra']))\n",
    "                                \n",
    "        data_bootstrap = []\n",
    "\n",
    "        for i in range(n_boot):\n",
    "\n",
    "                index_bootstrap = np.random.choice(index_halos, len(file['ra']))\n",
    "                \n",
    "                file_bootstrap = Table(file)[index_bootstrap]\n",
    "                \n",
    "                ds_mean_bootstrap = np.average(file_bootstrap['ds_single'], weights = file_bootstrap['W_l'], axis = 0)\n",
    "                \n",
    "                data_bootstrap.append(ds_mean_bootstrap)\n",
    "\n",
    "        data_bootstrap = np.array(data_bootstrap)\n",
    "\n",
    "        N = np.stack((data_bootstrap.astype(float)), axis = 1)\n",
    "        \n",
    "        cov_N = np.cov(N, bias = False,ddof=0)\n",
    "        \n",
    "        return cov_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covariance matrix\n",
    "n_boot = 200\n",
    "bootstrap_covariance_stack_true = stack_bootstrap_covariance_matrix( Stack_file_true, n_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance matrix for single cluster analysis\n",
    "For the single cluster analysis, we use the background galaxy catalog of one random cluster in our sample.\n",
    "## Jackknife covariance matrix\n",
    "We estimate the single excess surface density covariance matrix by the delete-1 Jackknife method : We split the sky area arround the cluster center in $N_{\\rm JK}$ regions, and we exclude one region at a time and re-estimate the excess surface density profile:, the covariance writes\n",
    "\n",
    "$$\n",
    "    (\\mathbf{C}^{\\rm JK})_{i,j} = \\frac{N_{\\rm JK} - 1}{N_{\\rm JK}}\\sum_{k = 0}^{N_{\\rm JK}}[\\Delta\\Sigma^k(R_i) - \\overline{\\Delta\\Sigma}(R_i)] [\\Delta\\Sigma^k(R_j) - \\overline{\\Delta\\Sigma}(R_j)]\n",
    "$$\n",
    "with the mean \n",
    "$$\n",
    "    \\overline{\\Delta\\Sigma}(R_i) = \\frac{1}{N_{\\rm JK}}\\sum_{k = 0}^{N_{\\rm JK}} \\Delta\\Sigma^k(R_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_jackknife_covariance_matrix(single_catalog = None, N_jk = None):\n",
    "    \n",
    "    r\"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    single_catalog : GalaxyCluster object\n",
    "        GalaxyCluster object of a single cluster \n",
    "    N_jk : int\n",
    "        number of jackknife regions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cov : array\n",
    "        Jackknife delete-1 covariance matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    phi = np.linspace(-np.pi, np.pi, N_jk + 1)\n",
    "    \n",
    "    phi_bin = binning(phi)\n",
    "    \n",
    "    gt = []\n",
    "    \n",
    "    mask_list_in = [(phi_[0] < single_catalog.galcat['phi'])*(single_catalog.galcat['phi'] < phi_[1]) for phi_ in phi_bin]\n",
    "    \n",
    "    for s, phi_ in enumerate(phi_bin):\n",
    "        \n",
    "        mask_in = mask_list_in[s]\n",
    "        \n",
    "        mask_out = np.invert(mask_in)\n",
    "        \n",
    "        #plt.scatter(single_catalog.galcat['ra'][mask_in], single_catalog.galcat['dec'][mask_in], s = 1)\n",
    "        \n",
    "        data_cut = single_catalog.galcat[mask_out]\n",
    "        \n",
    "        cl_cut_jack = clmm.GalaxyCluster('Stack', single_catalog.ra, single_catalog.dec, single_catalog.z, single_catalog.galcat[mask_out])\n",
    "        \n",
    "        ds, r, sum_weights, N_gal_per_bin = excess_surface_density(single_catalog = cl_cut_jack, radial_bin = radial_bin, sigma_c = 'sigma_c_true')\n",
    "        \n",
    "        gt.append(ds)\n",
    "\n",
    "    gt = np.array(gt)\n",
    "    \n",
    "    cov = np.cov(gt.T, bias = False)*(N_jk-1)**2/N_jk\n",
    "    \n",
    "    #plt.xlabel('ra')\n",
    "    #plt.ylabel('dec')\n",
    "    #plt.axis('equal')\n",
    "        \n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jack_single = 300\n",
    "jackknife_covariance_single_true = single_jackknife_covariance_matrix(single_catalog = GalaxyCluster_list[0], N_jk = n_jack_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted variance due to the shapenoise of galaxies is given by\n",
    "$$\n",
    "(\\sigma_{\\Delta\\Sigma}^{\\rm SN})^2 = \\frac{\\sigma_{\\rm shape}^2\\langle\\Sigma_{\\rm crit}(z_l, z_s)^2\\rangle}{N_{\\rm gal/bin}(R_i)}\n",
    "$$\n",
    "We can estimate the shapenoise variance and expect the total variance to be of the order of the shapenoise variance, as a validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize =(20, 5))\n",
    "cov = [jackknife_covariance_stack_true, bootstrap_covariance_stack_true,\n",
    "       sample_covariance_stack_true, jackknife_covariance_single_true]\n",
    "label = ['Stack : JK', 'Stack : boot', 'Stack : sample', 'Single : JK']\n",
    "for i in range(4):\n",
    "    ax[i].set_title(label[i], fontsize = 20)\n",
    "    im = ax[i].imshow(cov[i], vmin = 1e22, cmap = 'Reds', origin = 'lower')\n",
    "    plt.colorbar(im, ax=ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of the shapenoise variance\n",
    "N_gal_per_bin_stack = np.sum(np.array(Stack_file_true['N_gal_per_bin']), axis = 0)\n",
    "N_gal_per_bin_single = Stack_file_true['N_gal_per_bin'][0]\n",
    "sigma_c_power_2_av = np.mean(GalaxyCluster_list[0].galcat['sigma_c_true']**2)\n",
    "variance_shapenoise_stack = sigma_c_power_2_av*shapenoise**2/N_gal_per_bin_stack\n",
    "variance_shapenoise_single = sigma_c_power_2_av*shapenoise**2/N_gal_per_bin_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize = (10,15), sharex = True)\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "ax[0].loglog(r_mean_true, ds_mean_true, label = r'Stacked $\\Delta\\Sigma$')\n",
    "ax[0].loglog(r_mean_true, Stack_file_true['ds_single'][0], label = r'Single $\\Delta\\Sigma$')\n",
    "ax[1].loglog(r_mean_true, jackknife_covariance_stack_true.diagonal(), label = f'Stack JK, $N_{{JK}}$ : {n_jack}')\n",
    "ax[1].plot(r_mean_true, sample_covariance_stack_true.diagonal(), label = 'Stack Sample')\n",
    "ax[1].loglog(r_mean_true, bootstrap_covariance_stack_true.diagonal(), label = f'Stack Boot, $N_{{boot}}$ : {n_boot}')\n",
    "ax[1].loglog(r_mean_true, jackknife_covariance_single_true.diagonal(), label = f'Single JK, $N_{{JK}}$ : {n_jack_single}')\n",
    "ax[1].loglog(r_mean_true, variance_shapenoise_stack, '--k', label = r'Stack $(\\sigma_{\\Delta\\Sigma}^{\\rm SN})^2$')\n",
    "ax[1].loglog(r_mean_true, variance_shapenoise_single, '--b', label = r'Single $(\\sigma_{\\Delta\\Sigma}^{\\rm SN})^2$')\n",
    "ax[1].legend(frameon = False, fontsize = 12)\n",
    "ax[0].legend(frameon = False, fontsize = 20)\n",
    "ax[1].set_xlabel(r'$R\\ [Mpc]$', fontsize = 20)\n",
    "ax[1].set_ylabel(r'Variance $\\sigma_{\\Delta\\Sigma}^2$', fontsize = 20)\n",
    "ax[0].set_ylabel(r'Excess surface density $\\Delta\\Sigma$', fontsize = 20)\n",
    "ax[0].tick_params(axis='both', which = 'major', labelsize= 15)\n",
    "ax[1].tick_params(axis='both', which = 'major', labelsize= 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-clmmenv",
   "language": "python",
   "name": "conda-clmmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
