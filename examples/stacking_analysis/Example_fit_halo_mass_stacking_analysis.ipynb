{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we import some clmm modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import clmm\n",
    "\n",
    "import clmm.dataops\n",
    "from clmm.dataops import compute_tangential_and_cross_components, make_radial_profile, make_bins\n",
    "from clmm.galaxycluster import GalaxyCluster\n",
    "import clmm.utils as u\n",
    "from clmm import Cosmology\n",
    "from clmm.support import mock_data as mock\n",
    "cosmo = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)\n",
    "import numpy as np\n",
    "cosmo_clmm = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from numpy import random\n",
    "import chainconsumer\n",
    "import scipy\n",
    "\n",
    "clmm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we import `clmm` module's core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define `astropy` and `ccl` cosmology object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_cosmo = Cosmology(H0 = 71.0, Omega_dm0 = 0.265 - 0.0448, Omega_b0 = 0.0448, Omega_k0 = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moo = clmm.Modeling(massdef = 'critical', delta_mdef = 200, halo_profile_model = 'nfw')\n",
    "moo.set_cosmo(mock_cosmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the `Modeling` object `moo` to model the galaxy cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now define the type of desired profile : tangential reduced shear of Excess surface density for the stacking procedure by modifying the atribute. We use the method `Type` from the class to select DeltaSigma of reduced tangential shear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(corner): return [[corner[i],corner[i+1]] for i in range(len(corner)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator of the stacked excess surface density\n",
    "\n",
    "- The maximum likelihood estimator of the excess surface density in the radial bin $[R, R + \\Delta R[$ is defined by\n",
    "\n",
    "$$\\widehat{\\Delta\\Sigma}_+(R) = \\frac{1}{\\sum\\limits_{l,s = 1} w_{ls}}\n",
    "     \\sum\\limits_{l,s= 1}w_{ls}\\langle\\Sigma_{{\\rm crit}}(z_s, z_l)^{-1}\\rangle^{-1}\\epsilon_+^{s},$$ where $\\epsilon_+^{s}$ is the tangential ellipticity of the background galaxy with index $s$ (as source) relative to the dark matter halo position with index $l$ (as lens).\n",
    "     \n",
    "- The critical surface mass density expresses as $\\Sigma_{{\\rm crit}}(z_s, z_l) = \\frac{c^2}{4 \\pi G} \\frac{D_A(z_s)}{D_A(z_l) D_A(z_s, z_l)}$, where $D_A(z_l), D_A(z_s)$ are respectively the angular diameter distance to the lens and to the source in physical units, and $D_A(z_s, z_l)$ is the angular diameter distance between the lens and the source.\n",
    "\n",
    "\n",
    "- The average $\\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle$ is defined as $\\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle = \\int_{z_l + \\delta}^{+\\infty} d z_s\\ p_{\\rm photoz,s}(z_s)\\ \\Sigma_{\\rm crit}(z_s,z_l)^{-1}$, where $p_{\\rm photoz,s}$ is the photometric probability density function for the background galaxy with index $s$.\n",
    "\n",
    "This equation can be re-written as\n",
    "\n",
    "$$\\widehat{\\Delta\\Sigma}_+(R) = \\frac{1}{\\sum\\limits_{l = 1} W_{l}}\n",
    "     \\sum\\limits_{l,s= 1}W_{l}\\Delta\\Sigma_l(R)$$\n",
    "     \n",
    "where $$\\Delta\\Sigma_l(R) = \\frac{1}{\\sum\\limits_{s = 1} w_{ls}}\n",
    "     \\sum\\limits_{s= 1}w_{ls}\\langle\\Sigma_{{\\rm crit}}(z_s, z_l)^{-1}\\rangle^{-1}\\epsilon_+^{s}$$\n",
    "     \n",
    "and $$W_{l} = \\sum\\limits_{s= 1}w_{ls}$$\n",
    "\n",
    "### Weights $w_s$\n",
    "\n",
    "The quantities $w_{s}$ are the weights that maximise the sigmnal-to-noise ratio of the excess surface density estimator. They downweight the galaxies that are close in redshift to the cluster (where the lensing signal is weak). They include the lack of informations on both redshift and shape reconstruction for each background galaxies. \n",
    "\n",
    "- In the case where there is no error on the shape measurement (for the purpose of cosmoDC2 galaxies), the weight writes $w_{ls} = \\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle^{2}$,\n",
    "\n",
    "- In the case of true redshift $z_s$, the PDF reduces to a Dirac function centered at $z_s$, giving the average $\\langle\\Sigma_{\\rm crit}(z_l,z_s)^{-1}\\rangle = \\Sigma_{\\rm crit}(z_l,z_s)^{-1} $ and the weight $w_{s} = \\Sigma_{\\rm crit}(z_s,z_l)^{-2}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define some redshift interval and input mass interval to select GalaxyCluster object in preselected galaxy catalogs from `cosmoDC2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dark matter halo catalog for the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bin = [0.2,0.25]\n",
    "logm_bin = [14, 14.1] #Solar Mass\n",
    "logm_bin = np.array(logm_bin)\n",
    "n_catalogs = 50\n",
    "ngals = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define a set of selected cluster with given true masses ans true redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_m = 10**((logm_bin[1] - logm_bin[0])*np.random.random(n_catalogs) + logm_bin[0]) #in M_sun\n",
    "cluster_z = (z_bin[1] - z_bin[0])*np.random.random(n_catalogs) + z_bin[0]\n",
    "lnc = abs(np.log(4) + 0.1*np.random.randn(n_catalogs))\n",
    "concentration = np.exp(lnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cluster_m, concentration)\n",
    "plt.xlabel(r'$M_{\\rm 200c}$', fontsize = 20)\n",
    "plt.ylabel(r'$c_{\\rm 200c}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add each galaxy catalogs that correponds to binning criteria to the `shear` object through the `Add(self,'file_name')` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenoise = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_surface_density(single_catalog = None, radial_bin = None, sigma_c = None):\n",
    "    r\"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    \n",
    "    single_catalog : GalaxyCluster object\n",
    "    \n",
    "    radial_bin : liste\n",
    "        radial bins to evaluate the binned excess surface density\n",
    "    \n",
    "    sigma_c : string \n",
    "        column name in the single_catalog for the critical surface mass density\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    ds, r, sum_weights : array, array, array\n",
    "        the binned excess surface density, the binned radius, the sum of weights w_ls in each radial bin\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ds = np.zeros(len(radial_bin))\n",
    "    \n",
    "    r = np.zeros(len(radial_bin))\n",
    "    \n",
    "    sum_weights = np.zeros(len(radial_bin))\n",
    "    \n",
    "    for i, r_bin in enumerate(radial_bin):\n",
    "\n",
    "        mask = (single_catalog.galcat['R'] > r_bin[0])*(single_catalog.galcat['R'] < r_bin[1])\n",
    "        \n",
    "        w_ls = 1./single_catalog.galcat[sigma_c][mask]**2\n",
    "        \n",
    "        ds[i] = np.average(single_catalog.galcat['et'][mask]*single_catalog.galcat[sigma_c][mask], weights = w_ls)\n",
    "        \n",
    "        r[i] = np.average(single_catalog.galcat['R'][mask])\n",
    "        \n",
    "        sum_weights[i] = np.sum( w_ls )\n",
    "        \n",
    "    return ds, r, sum_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating background galaxy catalogs & estimating individual shear profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `clmm` to generate `n_catalogs` background galaxy catalogs for each clusters in the sample. We estimate the individual shear profile and store the individual data:\n",
    "\n",
    "- `ds_single`, `r_single` the single excess surface density extimated in the radial bins, and the average radius in each radial bins\n",
    "- `W_l` the sum of the weight `w_ls` for each lens-source pairs in each radial bins.\n",
    "We difine two possible weights `w_ls`, first using true redshifts of sources, second using the photometric redshift pdf for each sources as presented above. In the function `excess_surface_density`, the argument `sigma_c` will be computed using the true redshift of galaxy, of it will be given by $\\langle\\Sigma_{\\rm crit}(z_s,z_l)^{-1}\\rangle^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_photoz_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "GalaxyCluster = []\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    if i%10 == 0: print(i)\n",
    "    \n",
    "    noisy_data_z = mock.generate_galaxy_catalog(cluster_m[i], cluster_z[i], concentration[i], \n",
    "                                                cosmo, \n",
    "                                                zsrc = 'chang13', \n",
    "                                                Delta_SO=200, \n",
    "                                                massdef='critical',\n",
    "                                                halo_profile_model='nfw', \n",
    "                                                zsrc_min=cluster_z[i] + 0.1,\n",
    "                                                zsrc_max=3., \n",
    "                                                field_size=10., \n",
    "                                                shapenoise=shapenoise, \n",
    "                                                photoz_sigma_unscaled=0.05, \n",
    "                                                ngals=ngals)\n",
    "    \n",
    "    if use_photoz_weights == True:\n",
    "        \n",
    "        pzbins_constant = np.linspace(0,3,30)\n",
    "        \n",
    "        sigma_crit_1 = 1./mock_cosmo.eval_sigma_crit(cluster_z[i], pzbins_constant)\n",
    "\n",
    "        photoz_matrix = np.zeros([len(noisy_data_z), len(pzbins_constant)])\n",
    "        \n",
    "        r\"\"\"\n",
    "        I added this line since the photoz pdf is not computed on a constant redshift grid. simps integral can be optimized using the same x-axis for multiple integrals.\n",
    "        =================================================================================================================================================================\n",
    "        \"\"\"\n",
    "\n",
    "        for f in range(len(noisy_data_z['id'])):\n",
    "\n",
    "            photoz_matrix[f,:] = np.interp(pzbins_constant, noisy_data_z['pzbins'][f], noisy_data_z['pzpdf'][f])\n",
    "            \n",
    "        r\"\"\"\n",
    "        =================================================================================================================================================================\n",
    "        \"\"\"\n",
    "\n",
    "        unormed_integral = scipy.integrate.simps(photoz_matrix * sigma_crit_1, x = pzbins_constant, axis = 1)\n",
    "\n",
    "        norm = scipy.integrate.simps(photoz_matrix, x = pzbins_constant, axis = 1)\n",
    "\n",
    "        noisy_data_z['sigma_c_photoz'] = (unormed_integral/norm)**(-1)\n",
    "        \n",
    "    \n",
    "    cl = clmm.GalaxyCluster('mock_cluster', 0, 0, cluster_z[i], noisy_data_z)\n",
    "    \n",
    "    cl.compute_tangential_and_cross_components(geometry=\"flat\",\n",
    "                                              shape_component1='e1', shape_component2='e2', \n",
    "                                              tan_component='et', cross_component='ex',\n",
    "                                              add=True, \n",
    "                                              is_deltasigma = False, \n",
    "                                              cosmo = cosmo)\n",
    "    \n",
    "    cl.galcat['R'] = mock_cosmo.eval_da_z1z2(0,cluster_z[i])*cl.galcat['theta']\n",
    "    \n",
    "    ang, phi = clmm.dataops._compute_lensing_angles_flatsky(cl.ra, cl.dec, cl.galcat['ra'], cl.galcat['dec'])\n",
    "\n",
    "    cl.galcat['phi'] = np.array(phi)\n",
    "    \n",
    "    cl.galcat['sigma_c_true'] = mock_cosmo.eval_sigma_crit(cluster_z[i], cl.galcat['ztrue'])\n",
    "    \n",
    "    GalaxyCluster.append(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating single excess surface density profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\widehat{\\Delta\\Sigma}_+(R) = \\frac{1}{\\sum\\limits_{l = 1} W_{l}}\n",
    "     \\sum\\limits_{l,s= 1}W_{l}\\Delta\\Sigma_l(R)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bins = np.logspace(np.log10(0.4), np.log10(5), 20)\n",
    "radial_bin = [[new_bins[s],new_bins[s+1]] for s in range(len(new_bins)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ds_single', 'r_single', 'W_l','c', 'z', 'mass']\n",
    "\n",
    "Stack_file_true = {name : [] for name in names}\n",
    "\n",
    "Stack_file_photoz = {name : [] for name in names}\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    ds_single, r_single, sum_weights_single = excess_surface_density(single_catalog = GalaxyCluster[i], radial_bin = radial_bin, sigma_c = 'sigma_c_true')\n",
    "    \n",
    "    data_to_store = [ds_single, r_single, sum_weights_single, concentration[i], cluster_z[i], cluster_m[i]]\n",
    "    \n",
    "    for d, name in enumerate(names):\n",
    "        \n",
    "        Stack_file_true[name].append(data_to_store[d])\n",
    "        \n",
    "    ds_single, r_single, sum_weights_single = excess_surface_density(single_catalog = GalaxyCluster[i], radial_bin = radial_bin, sigma_c = 'sigma_c_photoz')\n",
    "    \n",
    "    data_to_store = [ds_single, r_single, sum_weights_single, concentration[i], cluster_z[i], cluster_m[i]]\n",
    "    \n",
    "    for d, name in enumerate(names):\n",
    "        \n",
    "        Stack_file_photoz[name].append(data_to_store[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked excess surface density profile : true z and photoz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the mean excess surface density profile `ds_mean` and the mean radius `r_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked excess surface density profile - true redshift case\n",
    "\n",
    "ds_mean_true = np.average(Stack_file_true['ds_single'], weights = Stack_file_true['W_l'], axis = 0)\n",
    "r_mean_true = np.average(Stack_file_true['r_single'], weights = None, axis = 0)\n",
    "\n",
    "#stacked excess surface density profile - photoz redshift case\n",
    "ds_mean_photoz = np.average(Stack_file_photoz['ds_single'], weights = Stack_file_photoz['W_l'], axis = 0)\n",
    "r_mean_photoz = np.average(Stack_file_photoz['r_single'], weights = None, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the sample covariance matrix defined as\n",
    "$$\n",
    "    (\\mathbf{C}^{\\rm sample})_{i,j} = \\frac{1}{N_{\\rm cluster} - 1}\\sum_{k = 0}^{N_{\\rm cluster}}[\\Delta\\Sigma^k(R_i) - \\overline{\\Delta\\Sigma}(R_i)] [\\Delta\\Sigma^k(R_j) - \\overline{\\Delta\\Sigma}(R_j)],\n",
    "$$\n",
    "and $\\overline{\\Delta\\Sigma}(R_i)$ is denoted by\n",
    "$$\n",
    "    \\overline{\\Delta\\Sigma}(R_i) = \\frac{1}{N_{\\rm cluster}}\\sum_{k = 0}^{N_{\\rm boot}} \\Delta\\Sigma^k(R_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covariance matrix\n",
    "sample_covariance_stack_true = np.cov(np.array(Stack_file_true['ds_single']).T)/n_catalogs\n",
    "sample_covariance_stack_photoz = np.cov(np.array(Stack_file_photoz['ds_single']).T)/n_catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single cluster : Generating background galaxy catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make standard comparison with the stack analysis, we use also consider a single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_data_z_single = mock.generate_galaxy_catalog(np.mean(cluster_m), np.mean(cluster_z), np.mean(concentration), cosmo, \n",
    "                                                zsrc = 'chang13', \n",
    "                                                Delta_SO=200, \n",
    "                                               massdef='critical',\n",
    "                                               halo_profile_model='nfw', zsrc_min=np.mean(cluster_z) + 0.1,\n",
    "                           zsrc_max=3., field_size=10., shapenoise=shapenoise, photoz_sigma_unscaled=0, ngals=ngals)\n",
    "\n",
    "cl_single = clmm.GalaxyCluster('mock_cluster', 0., 0., np.mean(cluster_z), noisy_data_z_single)\n",
    "\n",
    "cl_single.compute_tangential_and_cross_components(geometry=\"flat\",\n",
    "                                                  shape_component1='e1', shape_component2='e2', \n",
    "                                                  tan_component='et', cross_component='ex',\n",
    "                                                  add=True, is_deltasigma = False, cosmo = cosmo)\n",
    "\n",
    "cl_single.galcat['R'] = mock_cosmo.eval_da_z1z2(0,np.mean(cluster_z))*cl_single.galcat['theta']\n",
    "\n",
    "ang, phi = clmm.dataops._compute_lensing_angles_flatsky(cl_single.ra, cl_single.dec, cl_single.galcat['ra'], cl_single.galcat['dec'])\n",
    "\n",
    "cl_single.galcat['phi'] = np.array(phi)\n",
    "\n",
    "cl_single.galcat['sigma_c'] = mock_cosmo.eval_sigma_crit(np.mean(cluster_z), cl_single.galcat['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single cluster : excess surface density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excess surface density profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excess surface density profile\n",
    "ds_single,r_single,sum_weight_single = excess_surface_density(single_catalog = cl_single, radial_bin = radial_bin, sigma_c = 'sigma_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the single excess surface density covariance matrix by the delete-1 Jackknife method : We split the sky area arround the cluster center in $N_{\\rm JK}$ regions, and we exclude one region at a time and re-estimate the excess surface density profile:, the covariance writes\n",
    "\n",
    "$$\n",
    "    (\\mathbf{C}^{\\rm JK})_{i,j} = \\frac{N_{\\rm JK} - 1}{N_{\\rm JK}}\\sum_{k = 0}^{N_{\\rm JK}}[\\Delta\\Sigma^k(R_i) - \\overline{\\Delta\\Sigma}(R_i)] [\\Delta\\Sigma^k(R_j) - \\overline{\\Delta\\Sigma}(R_j)]\n",
    "$$\n",
    "with the mean \n",
    "$$\n",
    "    \\overline{\\Delta\\Sigma}(R_i) = \\frac{1}{N_{\\rm JK}}\\sum_{k = 0}^{N_{\\rm JK}} \\Delta\\Sigma^k(R_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackknife(single_catalog = None, N_jk = None):\n",
    "    \n",
    "    r\"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    single_catalog : GalaxyCluster object\n",
    "        GalaxyCluster object of a single cluster \n",
    "    N_jk : int\n",
    "        number of jackknife regions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cov : array\n",
    "        Jackknife delete-1 covariance matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    phi = np.linspace(-np.pi, np.pi, N_jk + 1)\n",
    "    \n",
    "    phi_bin = binning(phi)\n",
    "    \n",
    "    gt = []\n",
    "    \n",
    "    mask_list_in = [(phi_[0] < single_catalog.galcat['phi'])*(single_catalog.galcat['phi'] < phi_[1]) for phi_ in phi_bin]\n",
    "    \n",
    "    for s, phi_ in enumerate(phi_bin):\n",
    "        \n",
    "        mask_in = mask_list_in[s]\n",
    "        \n",
    "        mask_out = np.invert(mask_in)\n",
    "        \n",
    "        plt.scatter(single_catalog.galcat['ra'][mask_in], single_catalog.galcat['dec'][mask_in], s = 1)\n",
    "        \n",
    "        data_cut = single_catalog.galcat[mask_out]\n",
    "        \n",
    "        cl_cut_jack = clmm.GalaxyCluster('Stack', single_catalog.ra, single_catalog.dec, single_catalog.z, single_catalog.galcat[mask_out])\n",
    "        \n",
    "        ds, r, sum_weights = excess_surface_density(single_catalog = cl_cut_jack, radial_bin = radial_bin, sigma_c = 'sigma_c')\n",
    "        \n",
    "        gt.append(ds)\n",
    "\n",
    "    gt = np.array(gt)\n",
    "    \n",
    "    cov = np.cov(gt.T)*(N_jk-1)**2/N_jk\n",
    "    \n",
    "    plt.xlabel('ra')\n",
    "    plt.ylabel('dec')\n",
    "    plt.axis('equal')\n",
    "        \n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jackknife covariance matrix\n",
    "cov_single = jackknife(single_catalog = cl_single, N_jk = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize = (20,10))\n",
    "#axs[0,0].imshow(np.corrcoef(cov_ss), vmin = -1,, cmap = 'bwr', origin = 'lower')\n",
    "min_value = min(sample_covariance_stack_true.flatten())\n",
    "im = axs[0,0].imshow(cov_single, cmap = 'Reds', origin = 'lower', label = 'single-jackknife')\n",
    "plt.colorbar(im, ax=axs[0,0])\n",
    "axs[0,1].errorbar(r_single,ds_single, np.sqrt(cov_single.diagonal()), label = 'single')\n",
    "moo.set_concentration(concentration.mean())\n",
    "moo.set_mass(cluster_m.mean())\n",
    "axs[0,1].plot(r_single,moo.eval_excess_surface_density(r_single, cluster_z.mean()), label = 'fiducial')\n",
    "axs[0,1].set_ylim(0.3*1e13, 3.5*1e13)\n",
    "\n",
    "im = axs[1,0].imshow(sample_covariance_stack_true, cmap = 'Reds', origin = 'lower' ,label = 'stack-sample')\n",
    "plt.colorbar(im, ax=axs[1,0])\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "for i in range(n_catalogs):\n",
    "    axs[1,1].plot(Stack_file_true['r_single'][i], Stack_file_true['ds_single'][i], alpha = 0.01, c = 'grey', linewidth =20)\n",
    "axs[1,1].errorbar(r_mean_true,ds_mean_true, np.sqrt(sample_covariance_stack_true.diagonal()), linewidth = 3, label = 'stack - true z')\n",
    "axs[1,1].errorbar(r_mean_photoz,ds_mean_photoz, np.sqrt(sample_covariance_stack_photoz.diagonal()), linewidth = 3,fmt = '.', label = 'stack - photoz')\n",
    "axs[1,1].set_ylim(0.3*1e13, 3.5*1e13)\n",
    "\n",
    "axs[0,0].set_title('covariance matrix', fontsize = 20)\n",
    "axs[0,1].set_title('Excess surface density profile', fontsize = 20)\n",
    "for i in range(2):\n",
    "    axs[i,1].set_xlim(min(r_single), max(r_single))\n",
    "    axs[i,1].set_ylim(0.3*1e13, 3.5*1e13)\n",
    "    axs[i,1].set_xlabel('R [Mpc]', fontsize = 20)\n",
    "    axs[i,1].set_ylabel(r'$\\Delta\\Sigma$', fontsize = 20)\n",
    "    \n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i,j].legend(fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack VS single excess surface density : Fitting cluster mass and concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the cluster mass and concentration using the `emcee` package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnL(theta, r, ds, cov, z):\n",
    "    \n",
    "    logm, c = theta\n",
    "    \n",
    "    if c < 0.01: return -np.inf\n",
    "\n",
    "    moo.set_mass(10**logm)\n",
    "    \n",
    "    moo.set_concentration(c)\n",
    "    \n",
    "    y_th = moo.eval_excess_surface_density(r,z)\n",
    "    \n",
    "    delta = (y_th - ds)\n",
    "    \n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    \n",
    "    lnL_ds = -0.5*np.sum(delta*inv_cov.dot(delta))\n",
    "    \n",
    "    return lnL_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "initial = [np.log10(np.mean(cluster_m)),np.mean(concentration)]\n",
    "npath = 500\n",
    "nwalkers = 50\n",
    "pos = initial + 0.01 * np.random.randn(nwalkers, len(initial))\n",
    "nwalkers, ndim = pos.shape\n",
    "sampler_single = emcee.EnsembleSampler(nwalkers, ndim, lnL, args = (r_single, ds_single, cov_single, np.mean(cluster_z)))\n",
    "print('Single cluster : running...')\n",
    "sampler_single.run_mcmc(pos, npath, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stack true z : running...')\n",
    "sampler_stack_true = emcee.EnsembleSampler(nwalkers, ndim, lnL, args = (r_mean_true, ds_mean_true, sample_covariance_stack_true, np.mean(cluster_z)))\n",
    "sampler_stack_true.run_mcmc(pos, npath, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stack photoz : running...')\n",
    "sampler_stack_photoz = emcee.EnsembleSampler(nwalkers, ndim, lnL, args = (r_mean_photoz, ds_mean_photoz, sample_covariance_stack_photoz, np.mean(cluster_z)))\n",
    "sampler_stack_photoz.run_mcmc(pos, npath, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sample_stack_pre_cut = sampler_stack_true.get_chain(discard = 0, thin = 1, flat = True)\n",
    "flat_sample_single_pre_cut = sampler_single.get_chain(discard = 0, thin = 1, flat = True)\n",
    "plt.plot(flat_sample_single_pre_cut[:,0])\n",
    "plt.plot(flat_sample_stack_pre_cut[:,0])\n",
    "plt.ylabel(r'$\\log_{10}(M_{200c})$', fontsize= 20)\n",
    "plt.figure()\n",
    "plt.plot(flat_sample_single_pre_cut[:,1])\n",
    "plt.plot(flat_sample_stack_pre_cut[:,1])\n",
    "plt.ylabel(r'$c_{200c}$', fontsize= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = 300\n",
    "thin = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sample_stack_true = sampler_stack_true.get_chain(discard = discard, thin = thin, flat = True)\n",
    "flat_sample_stack_photoz = sampler_stack_photoz.get_chain(discard = discard, thin = thin, flat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sample_single = sampler_single.get_chain(discard = discard, thin = thin, flat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_sample_stack_true.shape, flat_sample_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r'$\\log_{10} M_{\\rm 200c}$',r'$c_{\\rm 200c}$', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "mean = [np.log10(np.mean(cluster_m)),np.mean(concentration)]\n",
    "c = ChainConsumer()\n",
    "c.add_chain(flat_sample_stack_true, parameters=labels, name = 'Stack - true z')\n",
    "c.add_chain(flat_sample_stack_photoz, parameters=labels, name = 'Stack - photoz')\n",
    "c.add_chain(flat_sample_single, parameters=labels, name = 'Single')\n",
    "c.configure (kde = True,colors=['b', 'r','g'], shade=True, shade_alpha=0.2, bar_shade=True, label_font_size=11, sigma2d=False, sigmas = [1, 2], spacing = 0.0, tick_font_size=11, usetex=False)\n",
    "plot_args = {}\n",
    "plot_args['truth'] = mean\n",
    "fig = c.plotter.plot(**plot_args)\n",
    "fig.set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mydesc",
   "language": "python",
   "name": "mydesc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
