{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e320d2bf-dce6-4a5b-ab38-04b0c948155f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using CLMM on Real Datasets\n",
    "\n",
    "_the LSST-DESC CLMM team_\n",
    "\n",
    "This notebook can be run on NERSC.\n",
    "\n",
    "Here we demonstrate how to run CLMM on real observational datasets. As an example, we use the data from the Dark Energy Survey (DES) public releases (DES Collaboration 2021, 2018). The catalogs can be accessed from the NOIRLab Astro Data Lab.\n",
    "\n",
    "The steps in this notebook includes:\n",
    "- [Setting things up](#Setup)\n",
    "- [Selecting a cluster](#Selecting_a_cluster)\n",
    "- [Downloading the published catalog at the cluster field](#Downloading_the_catalog)\n",
    "- [Loading the catalog into CLMM](#Loading_the_catalog)\n",
    "- [Running CLMM on the dataset](#Running_CLMM)\n",
    "\n",
    "Acknowledgement\n",
    "\n",
    "DES data: https://des.ncsa.illinois.edu/thanks\n",
    "\n",
    "Astro Data Lab: https://datalab.noirlab.edu/acknowledgements.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8f003-be65-44a3-8b0a-04749d1f7f13",
   "metadata": {},
   "source": [
    "<a id=\"Setup\"></a>\n",
    "## 1. Setup\n",
    "    \n",
    "We import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3503a1a-103f-48aa-b600-ef2d72de82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "from astropy.table import Table\n",
    "import pickle as pkl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e228cf4-f5f3-4b55-97b7-f1e022a5b29c",
   "metadata": {},
   "source": [
    "<a id=\"Selecting_a_cluster\"></a>\n",
    "## 2. Selecting a cluster\n",
    "\n",
    "We use the DES Y1 redMaPPer Catalogs (https://des.ncsa.illinois.edu/releases/y1a1/key-catalogs/key-redmapper) to select a list of high-richness (LAMBDA) galaxy clusters, which likely have high masses.\n",
    "\n",
    "Name | RA (deg) | DEC (deg) | Z_LAMBDA | LAMBDA | Note\n",
    "- | - | - | - | - | -\n",
    "RMJ025415.5-585710.7 | 43.564574 | -58.95297 | 0.429804 | 234.50368 | --\n",
    "RMJ051637.4-543001.6 | 79.155704 | -54.500456 | 0.30416065 | 195.06956 | ACO S520\n",
    "RMJ224851.8-443106.3 | 342.215897 | -44.518403 | 0.3514858 | 178.83827 | --\n",
    "...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684515e-e176-4b92-9509-0217ade681a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"Downloading_the_catalog\"></a>\n",
    "## 3. Downloading the catalog at the cluster field\n",
    "\n",
    "We consider RMJ051637.4-543001.6 (ACO S520) as an example.\n",
    "We can access the DES DR1 catalog from NOIRLab Data Lab (https://datalab.noirlab.edu/query.php?name=des_dr1.shape_metacal_riz_unblind). No registration is required.\n",
    "We make the query and download the catalogs in \"Query Interface\". \n",
    "We use `coadd_objects_id` to cross match the shape catalog, photo-z catalog, and photometry catalog. \n",
    "Since the cluster is at redshift about 0.3, a radius of 0.3 deg would be about a radial distance of 5 Mpc. \n",
    "The final catalog includes shape info, photo-z, and photometry. \n",
    "Here is an example of the query SQL command. \n",
    "The query could take a few minutes and the size of the catalog could be 3 MB (.csv). \n",
    "\n",
    "```\n",
    "SELECT P.mean_z, \n",
    "M.ra, M.dec,\n",
    "C.e1, C.e2, C.r11, C.r12, C.r21, C.r22 \n",
    "FROM des_dr1.photo_z as P\n",
    "INNER JOIN des_dr1.mof as M\n",
    "ON P.coadd_objects_id=M.coadd_objects_id\n",
    "INNER JOIN des_dr1.shape_metacal_riz_unblind as C\n",
    "ON P.coadd_objects_id=C.coadd_objects_id\n",
    "WHERE 't' = Q3C_RADIAL_QUERY(M.ra, M.dec,79.125000, -54.508333,0.3) \n",
    "--AND M.modest_class=1\n",
    "--AND M.mof_flux_r/M.mof_fluxerr_r>5\n",
    "--AND M.flags_r=0\n",
    "--AND M.flags_gold=0\n",
    "--AND M.mof_flags=0\n",
    "AND P.minchi2<4\n",
    "AND P.z_sigma<0.3\n",
    "AND C.flags_select=0\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4535f6-23b6-46e8-b9f8-7aeb899bfe7c",
   "metadata": {},
   "source": [
    "<a id=\"Loading_the_catalog\"></a>\n",
    "## 4. Loading the catalog into CLMM\n",
    "\n",
    "Once we have the catalog, we read in the catalog, make cuts on the catalog, and adjust column names to prepare for the analysis in CLMM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a04d966e-e00b-4e07-94d9-c3cc13b6183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.42 ms, sys: 0 ns, total: 5.42 ms\n",
      "Wall time: 4.43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Assume the downloaded catalog is at this path:\n",
    "filename = \"../../../acos520.csv\"\n",
    "catalog = filename.replace('.csv', '.pkl')\n",
    "if not Path(catalog).is_file():\n",
    "    data_0 = Table.read(filename, format=\"ascii.csv\")\n",
    "    pkl.dump(data_0, open(catalog,\"wb\"))\n",
    "else:\n",
    "    data_0 = pkl.load(open(catalog,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e10ef1-f035-462b-98c6-ceac1d32a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_0.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c53e2-aa5e-40e1-b46f-a955f0eb055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df10e3-771d-499e-b462-b72682902b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of responsivity (to reduce noise).\n",
    "# The diagonal terms are close.\n",
    "# The off-diagonal terms are much smaller.\n",
    "# We use the mean of the diagonal terms to reduce noise.\n",
    "print(np.mean(data_1['r11']), np.mean(data_1['r22']))\n",
    "print(np.mean(data_1['r12']), np.mean(data_1['r21']))\n",
    "r_diag = np.mean([np.mean(data_1['r11']), np.mean(data_1['r22'])])\n",
    "r_off_diag = np.mean([np.mean(data_1['r12']), np.mean(data_1['r21'])])\n",
    "print(r_diag, r_off_diag, r_off_diag/r_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98993f-2261-452b-8515-a2c64d52c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust column names.\n",
    "def adjust_column_names(catalog_in):\n",
    "    # We consider a map between new and old column names.\n",
    "    # Note we have considered shear calibration here.\n",
    "    column_name_map = {\n",
    "        \"ra\": \"ra\",\n",
    "        \"dec\": \"dec\",\n",
    "        'z': \"mean_z\", \n",
    "        \"e1\": \"e1\",\n",
    "        \"e2\": \"e2\",\n",
    "    }\n",
    "    \n",
    "    catalog_out = Table()\n",
    "    for i in column_name_map:\n",
    "        catalog_out[i] = catalog_in[column_name_map[i]]\n",
    "    \n",
    "    catalog_out[\"e1\"] /= r_diag\n",
    "    catalog_out[\"e2\"] /= r_diag\n",
    "    \n",
    "    return catalog_out\n",
    "\n",
    "data_2 = adjust_column_names(data_1)\n",
    "\n",
    "select = (data_2[\"e1\"]**2 + data_2[\"e2\"]**2 <=1.)\n",
    "print(np.sum(~select) )\n",
    "data_2 = data_2[select]\n",
    "\n",
    "data_2[\"id\"] = range(len(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d599e88-8a45-4b44-a46a-f2c00d86bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some figures for visualization.\n",
    "def make_plots(catalog_in):\n",
    "    # Scatter plot\n",
    "    plt.figure()\n",
    "    plt.scatter(catalog_in[\"ra\"], catalog_in[\"dec\"], c=catalog_in['z'], s=1., alpha=0.2)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"ra\")\n",
    "    plt.ylabel(\"dec\")\n",
    "    plt.title(\"z\")\n",
    "    \n",
    "    # Histogram\n",
    "    plt.figure()\n",
    "    plt.hist(catalog_in['z'], bins=20)\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel(\"count\")\n",
    "    \n",
    "    # Relation\n",
    "    plt.figure()\n",
    "    plt.plot(catalog_in[\"e1\"], catalog_in[\"e2\"], ',')\n",
    "    plt.xlabel(\"e1\")\n",
    "    plt.ylabel(\"e2\")\n",
    "\n",
    "make_plots(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d6d00-f18e-4e27-969b-f02f13032713",
   "metadata": {},
   "source": [
    "<a id=\"Running_CLMM\"></a>\n",
    "## 5. Running CLMM on the dataset\n",
    "We use the functions similar to `examples/Paper_v1.0/gt_and_use_case.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b465d-9e1c-4b63-a43e-eeea6139b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmm import Cosmology\n",
    "cosmo = Cosmology(H0=70.0, Omega_dm0=0.27-0.045, Omega_b0=0.045, Omega_k0=0.0)\n",
    "\n",
    "# We consider RMJ051637.4-543001.6 (ACO S520)\n",
    "cluster_z = 0.30416065 # Cluster redshift\n",
    "cluster_ra = 79.155704 # Cluster Ra in deg\n",
    "cluster_dec = -54.500456 # Cluster Dec in deg\n",
    "\n",
    "obs_galaxies = data_2\n",
    " \n",
    "obs_galaxies = obs_galaxies[(obs_galaxies['z']>(cluster_z+0.1))]\n",
    "\n",
    "obs_galaxies['id'] = np.arange(len(obs_galaxies))\n",
    "\n",
    "# Put galaxy values on arrays.\n",
    "gal_ra = obs_galaxies['ra'] # Galaxies Ra in deg\n",
    "gal_dec = obs_galaxies['dec'] # Galaxies Dec in deg\n",
    "gal_e1 = obs_galaxies['e1'] # Galaxies elipticipy 1\n",
    "gal_e2 = obs_galaxies['e2'] # Galaxies elipticipy 2\n",
    "gal_z = obs_galaxies['z'] # Galaxies observed redshift\n",
    "gal_id = obs_galaxies['id'] # Galaxies ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f44ef-ee3b-490b-8d54-4d433b50a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shear profile\n",
    "# Using/testing clmm.dataops functions (details are in the notebook for Paper V1.0).\n",
    "# This step is for testing the CLMM functionalities; we will not use the result in later steps.\n",
    "\n",
    "import clmm.dataops as da\n",
    "# Convert elipticities into shears.\n",
    "gal_ang_dist, gal_gt, gal_gx = da.compute_tangential_and_cross_components(cluster_ra, cluster_dec,\n",
    "                                                                          gal_ra, gal_dec,\n",
    "                                                                          gal_e1, gal_e2,\n",
    "                                                                          )\n",
    "\n",
    "# Measure profile.\n",
    "field_size = 6 # Mpc box size\n",
    "\n",
    "profile = da.make_radial_profile([gal_gt, gal_gx, gal_z],\n",
    "                                 gal_ang_dist, \"radians\", \"Mpc\",\n",
    "                                 bins=da.make_bins(0.2, field_size/2., 10),\n",
    "                                 cosmo=cosmo,\n",
    "                                 z_lens=cluster_z,\n",
    "                                 include_empty_bins=False)\n",
    "print(f'Profile table has columns: {\", \".join(profile.colnames)},')\n",
    "print('where p_(0, 1, 2) = (gt, gx, z)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42573e50-85cc-4977-baf4-723711792c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the GalaxyCluster object.\n",
    "\n",
    "import clmm\n",
    "from clmm.utils import convert_units\n",
    "\n",
    "# Create a GCData with the galaxies.\n",
    "galaxies = clmm.GCData([gal_ra, gal_dec, gal_e1, gal_e2, gal_z, gal_id],\n",
    "                      names=['ra', 'dec', 'e1', 'e2', 'z', 'id'])\n",
    "\n",
    "# Create a GalaxyCluster.\n",
    "cluster = clmm.GalaxyCluster(\"Name of cluster\", cluster_ra, cluster_dec,\n",
    "                                   cluster_z, galaxies)\n",
    "\n",
    "# Convert elipticities into shears for the members.\n",
    "cluster.compute_tangential_and_cross_components()\n",
    "print(cluster.galcat.colnames)\n",
    "\n",
    "# Measure profile and add profile table to the cluster. \n",
    "# This is just for showing the functionality.\n",
    "seps = convert_units(cluster.galcat['theta'], 'radians', 'Mpc', cluster.z, cosmo)\n",
    "\n",
    "cluster.make_radial_profile(bins=da.make_bins(0.4, 4.0, 7, method='evenlog10width'),\n",
    "                            bin_units=\"Mpc\",\n",
    "                            cosmo=cosmo,\n",
    "                            include_empty_bins=False,\n",
    "                            gal_ids_in_bins=True,\n",
    "                           )\n",
    "print(cluster.profile.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ebc79-67e8-43da-ad89-ca96666f541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('Paper_v1.0')\n",
    "from paper_formating import prep_plot\n",
    "fig = prep_plot(figsize=(9, 9))\n",
    "ax = fig.add_axes((0, 0, 1, 1))\n",
    "errorbar_kwargs = dict(linestyle='', marker='o',\n",
    "    markersize=1, elinewidth=.5, capthick=.5)\n",
    "ax.errorbar(cluster.profile['radius'], cluster.profile['gt'],\n",
    "             cluster.profile['gt_err'], c='k', **errorbar_kwargs)\n",
    "ax.set_xlabel('r [Mpc]', fontsize = 10)\n",
    "ax.set_ylabel(r'$g_t$', fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e51a27-936a-48f5-abc7-120cff89b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical predictions\n",
    "\n",
    "# Model relying on the overall redshift distribution of the sources (WtG III Applegate et al. 2014).\n",
    "\n",
    "z_inf = 1000\n",
    "dl_inf = cosmo.eval_da_z1z2(cluster_z, z_inf)\n",
    "d_inf = cosmo.eval_da(z_inf)\n",
    "concentration = 4.\n",
    "\n",
    "def betas(z):\n",
    "    dls = cosmo.eval_da_z1z2(cluster_z, z)\n",
    "    ds = cosmo.eval_da(z)\n",
    "    return dls * d_inf / (ds * dl_inf)\n",
    "\n",
    "def predict_reduced_tangential_shear_redshift_distribution(profile, logm):\n",
    "\n",
    "    bs_mean = np.mean(betas(cluster.galcat['z'])) \n",
    "    bs2_mean = np.mean(betas(cluster.galcat['z'])**2)\n",
    "\n",
    "    gamma_t_inf = clmm.compute_tangential_shear(\n",
    "            r_proj=profile['radius'], # Radial component of the profile\n",
    "            mdelta=10**logm, # Mass of the cluster [M_sun]\n",
    "            cdelta=concentration, # Concentration of the cluster\n",
    "            z_cluster=cluster_z, # Redshift of the cluster\n",
    "            z_source=z_inf, # Redshift value at infinity\n",
    "            cosmo=cosmo,\n",
    "            delta_mdef=200,\n",
    "            massdef='critical', # For M200c\n",
    "            halo_profile_model='nfw')\n",
    "    convergence_inf = clmm.compute_convergence(\n",
    "            r_proj=profile['radius'], # Radial component of the profile\n",
    "            mdelta=10**logm, # Mass of the cluster [M_sun]\n",
    "            cdelta=concentration, # Concentration of the cluster\n",
    "            z_cluster=cluster_z, # Redshift of the cluster\n",
    "            z_source=z_inf, # Redshift value at infinity\n",
    "            cosmo=cosmo,\n",
    "            delta_mdef=200,\n",
    "            massdef='critical',\n",
    "            halo_profile_model='nfw')\n",
    "        \n",
    "    return bs_mean*gamma_t_inf/(1-(bs2_mean/bs_mean)*convergence_inf)\n",
    "\n",
    "\n",
    "# Model using individual redshift and radial information, to compute the averaged shear in each radial bin, based on the galaxies actually present in that bin.\n",
    "cluster.galcat['theta_mpc'] = convert_units(cluster.galcat['theta'], 'radians', 'mpc',cluster.z, cosmo)\n",
    "\n",
    "def predict_reduced_tangential_shear_individual_redshift(profile, logm):\n",
    "    return np.array([np.mean(\n",
    "        clmm.compute_reduced_tangential_shear(\n",
    "            # Radial component of each source galaxy inside the radial bin\n",
    "            r_proj=cluster.galcat[radial_bin['gal_id']]['theta_mpc'],\n",
    "            mdelta=10**logm, # Mass of the cluster [M_sun]\n",
    "            cdelta=concentration, # Concentration of the cluster\n",
    "            z_cluster=cluster_z, # Redshift of the cluster\n",
    "            # Redshift value of each source galaxy inside the radial bin\n",
    "            z_source=cluster.galcat[radial_bin['gal_id']]['z'],\n",
    "            cosmo=cosmo,\n",
    "            delta_mdef=200,\n",
    "            massdef='critical',\n",
    "            halo_profile_model='nfw'\n",
    "        )) for radial_bin in profile])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83799a1-5122-4e89-b67d-8f691ff9d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass fitting\n",
    "\n",
    "mask_for_fit = cluster.profile['n_src'] > 2\n",
    "data_for_fit = cluster.profile[mask_for_fit]\n",
    "\n",
    "from clmm.support.sampler import fitters\n",
    "def fit_mass(predict_function):\n",
    "    popt, pcov = fitters['curve_fit'](predict_function,\n",
    "        data_for_fit, \n",
    "        data_for_fit['gt'], \n",
    "        data_for_fit['gt_err'], bounds=[10.,17.])\n",
    "    logm, logm_err = popt[0], np.sqrt(pcov[0][0])\n",
    "    return {'logm':logm, 'logm_err':logm_err,\n",
    "            'm': 10**logm, 'm_err': (10**logm)*logm_err*np.log(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59846f-a380-449d-a30b-5261085a34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_redshift_distribution = fit_mass(predict_reduced_tangential_shear_redshift_distribution)\n",
    "fit_individual_redshift = fit_mass(predict_reduced_tangential_shear_individual_redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b8ac6-7294-4e85-97ac-28b62512be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best fit mass for N(z) model                     = {fit_redshift_distribution[\"m\"]:.3e} +/- {fit_redshift_distribution[\"m_err\"]:.3e} Msun')\n",
    "print(f'Best fit mass for individual redshift and radius = {fit_individual_redshift[\"m\"]:.3e} +/- {fit_individual_redshift[\"m_err\"]:.3e} Msun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e71bf0-5aa8-4c48-8042-bd461430291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the results.\n",
    "def get_predicted_shear(predict_function, fit_values):\n",
    "    gt_est = predict_function(data_for_fit, fit_values['logm'])\n",
    "    gt_est_err = [predict_function(data_for_fit, fit_values['logm']+i*fit_values['logm_err'])\n",
    "                          for i in (-3, 3)]\n",
    "    return gt_est, gt_est_err\n",
    "\n",
    "gt_redshift_distribution, gt_err_redshift_distribution =  get_predicted_shear(predict_reduced_tangential_shear_redshift_distribution, fit_redshift_distribution)\n",
    "gt_individual_redshift, gt_err_individual_redshift =  get_predicted_shear(predict_reduced_tangential_shear_individual_redshift, fit_individual_redshift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e95d3-bc69-4cf7-8ac0-1788b74fe5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_redshift_distribution_dof = np.sum((gt_redshift_distribution-data_for_fit['gt'])**2/(data_for_fit['gt_err'])**2)/(len(data_for_fit)-1)\n",
    "chi2_individual_redshift_dof = np.sum((gt_individual_redshift-data_for_fit['gt'])**2/(data_for_fit['gt_err'])**2)/(len(data_for_fit)-1)\n",
    "\n",
    "print(f'Reduced chi2 (N(z) model) = {chi2_redshift_distribution_dof}')\n",
    "print(f'Reduced chi2 (individual (R,z) model) = {chi2_individual_redshift_dof}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89c28a-c19d-4022-83ce-32c113d2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "fig = prep_plot(figsize=(9 , 9))\n",
    "gt_ax = fig.add_axes((.25, .42, .7, .55))\n",
    "gt_ax.errorbar(data_for_fit['radius'],data_for_fit['gt'], data_for_fit['gt_err'],\n",
    "             c='k',\n",
    "            **errorbar_kwargs)\n",
    "\n",
    "# Points in grey have not been used for the fit.\n",
    "gt_ax.errorbar(cluster.profile['radius'][~mask_for_fit], cluster.profile['gt'][~mask_for_fit],\n",
    "               cluster.profile['gt_err'][~mask_for_fit], \n",
    "               c='grey',**errorbar_kwargs)\n",
    "\n",
    "pow10 = 14\n",
    "mlabel = lambda name, fits: fr'$M_{{fit}}^{{{name}}} = {fits[\"m\"]/10**pow10:.3f}\\pm{fits[\"m_err\"]/10**pow10:.3f}\\times 10^{{{pow10}}} M_\\odot$'\n",
    "\n",
    "# The model for the 1st method.\n",
    "gt_ax.loglog(data_for_fit['radius'], gt_redshift_distribution,'-C1', \n",
    "           label=mlabel('N(z)', fit_redshift_distribution),\n",
    "             lw=.5)\n",
    "gt_ax.fill_between(data_for_fit['radius'], *gt_err_redshift_distribution, lw=0, color='C1', alpha=.2)\n",
    "\n",
    "# The model for the 2nd method.\n",
    "gt_ax.loglog(data_for_fit['radius'], gt_individual_redshift,'-C2', \n",
    "           label=mlabel('z,R', fit_individual_redshift),\n",
    "             lw=.5)\n",
    "gt_ax.fill_between(data_for_fit['radius'], *gt_err_individual_redshift, lw=0, color='C2', alpha=.2)\n",
    "\n",
    "\n",
    "gt_ax.set_ylabel(r'$g_t$', fontsize = 8)\n",
    "gt_ax.legend(fontsize=6)\n",
    "gt_ax.set_xticklabels([])\n",
    "gt_ax.tick_params('x', labelsize=8)\n",
    "gt_ax.tick_params('y', labelsize=8)\n",
    "\n",
    "\n",
    "errorbar_kwargs2 = {k:v for k, v in errorbar_kwargs.items() if 'marker' not in k}\n",
    "errorbar_kwargs2['markersize'] = 3\n",
    "errorbar_kwargs2['markeredgewidth'] = .5\n",
    "res_ax = fig.add_axes((.25, .2, .7, .2))\n",
    "delta = (cluster.profile['radius'][1]/cluster.profile['radius'][0])**.25\n",
    "\n",
    "\n",
    "res_ax.errorbar(data_for_fit['radius'], data_for_fit['gt']/gt_redshift_distribution-1,\n",
    "                yerr=data_for_fit['gt_err']/gt_redshift_distribution, marker='s', c='C1', **errorbar_kwargs2)\n",
    "errorbar_kwargs2['markersize'] = 3\n",
    "errorbar_kwargs2['markeredgewidth'] = .5\n",
    "\n",
    "res_ax.errorbar(data_for_fit['radius']*delta, data_for_fit['gt']/gt_individual_redshift-1,\n",
    "                yerr=data_for_fit['gt_err']/gt_individual_redshift, marker='*', c='C2', **errorbar_kwargs2)\n",
    "res_ax.set_xlabel(r'$R$ [Mpc]', fontsize = 8)\n",
    "\n",
    "res_ax.set_ylabel(r'$g_t^{data}/g_t^{mod.}-1$', fontsize = 8)\n",
    "res_ax.set_xscale('log')\n",
    "\n",
    "res_ax.set_ylim(-5,5)\n",
    "res_ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "res_ax.tick_params('x', labelsize=8)\n",
    "res_ax.tick_params('y', labelsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566c5ca-913f-4a76-bdf5-6f7d6dcdf7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-clmmenv",
   "language": "python",
   "name": "conda-clmmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
