{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a WL mass using `clmm`\n",
    "\n",
    "_the LSST-DESC CLMM team_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import clmm.polaraveraging as pa\n",
    "import clmm.galaxycluster as gc\n",
    "import clmm.modeling as modeling\n",
    "import clmm\n",
    "import sys\n",
    "# This notebook must be run from the  \n",
    "sys.path.append('./support')\n",
    "import mock_data as mock\n",
    "from numpy import random\n",
    "from scipy import optimize as spo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a true cosmology\n",
    "# NB: need to cclify the astropy cosmology for generating the mock data (depends on modeling.py). \n",
    "# However, this is the astropy cosmology object that will need to be used on the data side, \n",
    "# profileaveraging.py (data side)\n",
    "\n",
    "mock_cosmo = FlatLambdaCDM(H0=70, Om0=0.27, Ob0=0.045)\n",
    "# cclify allows access to the cosmo parameter the CCL way, but it is NOT a CCL cosmology object,\n",
    "# but simply a dictionary\n",
    "# mock_cosmo_ccl = clmm.cclify_astropy_cosmo(mock_cosmo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define toy cluster parameters\n",
    "\n",
    "cosmo = mock_cosmo\n",
    "cluster_id = \"Awesome_cluster\"\n",
    "cluster_m = 1.e15\n",
    "cluster_z = 0.3\n",
    "src_z = 0.8\n",
    "concentration = 4\n",
    "ngals = 10000\n",
    "Delta = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make mock data object\n",
    "\n",
    "ideal_data  = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration,\n",
    "                                          cosmo, ngals, Delta, src_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a clmm.GalaxyCluster object\n",
    "# NB: mock data puts galaxy clusters in (0,0)\n",
    "\n",
    "cluster_ra = 0.0\n",
    "cluster_dec = 0.0\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec, \n",
    "                               cluster_z, ideal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the clmm.GalaxyCluster object\n",
    "\n",
    "gc_object.save('mock_GC.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clmm.GalaxyCluster object\n",
    "\n",
    "cl = clmm.load_cluster('mock_GC.pkl')\n",
    "print(\"Cluster info = ID:\", cl.unique_id, \"; ra:\", cl.ra, \"; dec:\", cl.dec, \"; z_l :\", cl.z)\n",
    "print (\"The number of source galaxies is :\", len(cl.galcat))\n",
    "\n",
    "ra_l = cl.ra\n",
    "dec_l = cl.dec\n",
    "z = cl.z\n",
    "e1 = cl.galcat['e1']\n",
    "e2 = cl.galcat['e2']\n",
    "ra_s = cl.galcat['ra']\n",
    "dec_s = cl.galcat['dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "fsize = 15\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "hb = fig.gca().hexbin(ra_s, dec_s, gridsize=50)\n",
    "\n",
    "cb = fig.colorbar(hb)\n",
    "cb.set_label('Number of sources in bin', fontsize=fsize)\n",
    "\n",
    "plt.gca().set_xlabel(r'$\\Delta RA$', fontsize=fsize)\n",
    "plt.gca().set_ylabel(r'$\\Delta Dec$', fontsize=fsize)\n",
    "plt.gca().set_title('Source Galaxies', fontsize=fsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tangential shear, cross shear for each source galaxy in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta, g_t , g_x = pa.compute_shear(cl, geometry = \"flat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "fig.gca().loglog(theta, g_t, '.')\n",
    "plt.ylabel(\"reduced shear\", fontsize=fsize)\n",
    "plt.xlabel(\"angular distance [deg?]\", fontsize=fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the binned profile\n",
    "\n",
    "Using 2 different binnings to highlight the impact on the reconstructed mass when doing naive fitting (not accounting for the binning in the model estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define bins\n",
    "bin_edges1 = pa.make_bins(0.01, 3.7, 50)\n",
    "bin_edges2 = pa.make_bins(0.01, 3.7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "res1 = pa.make_shear_profile(cl, \"radians\",\"Mpc\", bins=bin_edges1, cosmo = cosmo)\n",
    "res2 = pa.make_shear_profile(cl, \"radians\",\"Mpc\", bins=bin_edges2, cosmo = cosmo)\n",
    "\n",
    "fig.gca().loglog(res1['radius'], res1['gt'], '.', label='50 bins')\n",
    "fig.gca().loglog(res2['radius'], res2['gt'], '+', markersize=15, label='10 bins')\n",
    "plt.legend(fontsize=fsize)\n",
    "gt_profile1 = res1['gt']\n",
    "r1 = res1['radius']\n",
    "\n",
    "gt_profile2 = res2['gt']\n",
    "r2 = res2['radius']\n",
    "\n",
    "plt.gca().set_title(r'Binned shear of source galaxies', fontsize=fsize)\n",
    "plt.gca().set_xlabel(r'$r\\;(Mpc\\;\\;or\\;\\;h^{-1}\\,Mpc?)$', fontsize=fsize)\n",
    "plt.gca().set_ylabel(r'$g_t$', fontsize=fsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now the galaxy cluster as a profile attribute\n",
    "cl.profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct model\n",
    "\n",
    "Simply estimating the model at the bin location. In that case, the mass reconstruction is dependent on the binning. Future developement would be to take the average of the model inside the bin instead, which should solve this issue for this set of ideal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select density profile parametrization and parameter values\n",
    "\n",
    "# NB: the data are in physical Mpc, but modeling.py works in Mpc/h. So here, to build the model from bin positions, \n",
    "# we need to multiply the distance by h.\n",
    "\n",
    "def nfw_to_shear_profile1(logm):\n",
    "    m = 10.**logm\n",
    "    gt_model = clmm.predict_reduced_tangential_shear(r1*cosmo.h, m, concentration, cluster_z, src_z, cosmo, \n",
    "                                                     Delta=200, halo_profile_parameterization='nfw')\n",
    "    return sum((gt_model - gt_profile1) **2)\n",
    "\n",
    "def nfw_to_shear_profile2(logm):\n",
    "    m = 10.**logm\n",
    "    gt_model = clmm.predict_reduced_tangential_shear(r2*cosmo.h, m, concentration, cluster_z, src_z, cosmo, \n",
    "                                                     Delta=200, halo_profile_parameterization='nfw')\n",
    "    return sum((gt_model - gt_profile2) **2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit for mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize to find the best-fit mass\n",
    "\n",
    "logm_0 = random.uniform(13., 17., 1)[0]\n",
    "logm_est1 = spo.minimize(nfw_to_shear_profile1, logm_0).x\n",
    "logm_est2 = spo.minimize(nfw_to_shear_profile2, logm_0).x\n",
    "m_est1 = 10.**logm_est1\n",
    "m_est2 = 10.**logm_est2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est1, m_est2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rr = np.logspace(-2,np.log10(5),100)\n",
    "\n",
    "# NB: the data are in physical Mpc, but modeling.py works in Mpc/h. So here, to build the model from bin positions, \n",
    "# we need to multiply the distance by h.\n",
    "\n",
    "gt_model1 = clmm.predict_reduced_tangential_shear(rr*cosmo.h, m_est1, concentration, cluster_z,src_z, cosmo,\n",
    "                                                  Delta=200, halo_profile_parameterization='nfw')\n",
    "\n",
    "gt_model2 = clmm.predict_reduced_tangential_shear(rr*cosmo.h, m_est2, concentration, cluster_z, src_z, cosmo,\n",
    "                                                  Delta=200, halo_profile_parameterization='nfw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "fig.gca().scatter(r1, gt_profile1, label='mock data, M_input = %.3e Msun'%cluster_m)\n",
    "fig.gca().plot(rr, gt_model1, label = 'best fit model, M_fit=%.3e'%m_est1, color='orange')\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('R [Mpc]', fontsize=fsize)\n",
    "plt.ylabel('reduced tangential shear', fontsize=fsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "fig.gca().scatter(r2, gt_profile2, label='mock data, M_input = %.3e Msun'%cluster_m)\n",
    "fig.gca().plot(rr, gt_model2, label = 'best fit model, M_fit=%.3e Msun'%m_est2, color='orange')\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "\n",
    "plt.legend(fontsize=fsize)\n",
    "plt.xlabel('R [Mpc]', fontsize=fsize)\n",
    "plt.ylabel('reduced tangential shear', fontsize=fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup some files used for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('mock_GC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
